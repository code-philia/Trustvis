

import torch
import numpy as np
from sklearn.neighbors import NearestNeighbors

from sklearn.metrics.pairwise import euclidean_distances
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans,SpectralClustering,AgglomerativeClustering
from sklearn.metrics import pairwise_distances



from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

class SkeletonGenerator:
    """SkeletonGenerator except allows for generate skeleton"""
    def __init__(self, data_provider, epoch, interval=25,base_num_samples=10):
        """
        interval: int : layer number of the radius
        """
        self.data_provider = data_provider
        self.epoch = epoch
        self.interval = interval
        self.base_num_samples= base_num_samples
       
    def skeleton_gen(self):
        torch.manual_seed(0)  # freeze the radom seed
        torch.cuda.manual_seed_all(0)

        # Set the random seed for numpy
        np.random.seed(0)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        train_data=self.data_provider.train_representation(epoch=self.epoch)
        train_data = torch.Tensor(train_data)
        center = train_data.mean(dim=0)
        # calculate the farest distance
        radius = ((train_data - center)**2).sum(dim=1).max().sqrt()
        # print("radius,radius",radius)

        # min_radius_log = np.log10(1e-3)
        # max_radius_log = np.log10(radius.item())
        # # *****************************************************************************************
        # # generate 100 points in log space 
        # radii_log = np.linspace(max_radius_log, min_radius_log, self.interval)
        # # convert back to linear space
        # radii = 10 ** radii_log

        # generate points in log space
        # generate points in linear space
        radii = self.create_decreasing_array(1e-3,radius.item(), self.interval)
        epsilon = 1e-2
        train_data_distances = ((train_data - center)**2).sum(dim=1).sqrt().cpu().detach().numpy()
        # calculate the number of samples for each radius

        num_samples_per_radius_l = []
        for r in radii:
            close_points_indices = np.where(np.abs(train_data_distances - r) < epsilon)[0]
            close_points = train_data[close_points_indices].cpu().detach().numpy()
            print("len()",r, len(close_points))
            # calculate the log surface area for the current radius
            # convert it back to the original scale
            # calculate the number of samples
            base_num_samples = len(close_points) + 1
            num_samples = int(base_num_samples * r // 4)
            num_samples_per_radius_l.append(num_samples)
        

        # *****************************************************************************************

        # radii = [radius*1.1, radius, radius / 2, radius / 4, radius / 10, 1e-3]  # radii at which to sample points
        # # num_samples_per_radius_l = [500, 500, 500, 500, 500, 500]  # number of samples per radius
        # aaa = 500
        # num_samples_per_radius_l = [aaa, aaa, aaa, aaa, aaa, aaa]  # number of samples per radius
        print("num_samples_per_radius_l",radii)
        print("num_samples_per_radssius_l",num_samples_per_radius_l)
        # list to store samples at all radii
        high_bom_samples = []

        for i in range(len(radii)):
            r = radii[i]

            num_samples_per_radius = num_samples_per_radius_l[i]
            # sample points on the sphere with radius r
            samples = torch.randn(num_samples_per_radius, 512)
            samples = samples / samples.norm(dim=1, keepdim=True) * r

            high_bom_samples.append(samples)

            # concatenate samples from all radii
            high_bom = torch.cat(high_bom_samples, dim=0)

            high_bom = high_bom.cpu().detach().numpy()

        print("shape", high_bom.shape)
        
        # calculate the distance of each training point to the center


        # for each radius, find the training data points close to it and add them to the high_bom
        epsilon = 1e-3  # the threshold for considering a point is close to the radius
        for r in radii:
            close_points_indices = np.where(np.abs(train_data_distances - r) < epsilon)[0]
            close_points = train_data[close_points_indices].cpu().detach().numpy()
            high_bom = np.concatenate((high_bom, close_points), axis=0)

      
        return high_bom
    
    def skeleton_gen_union(self):
        torch.manual_seed(0)  # freeze the radom seed
        torch.cuda.manual_seed_all(0)

        # Set the random seed for numpy
        np.random.seed(0)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        train_data=self.data_provider.train_representation(epoch=self.epoch)
        # border_data = self.data_provider.border_representation(epoch=self.epoch)
        # train_data = np.concatenate((train_data,border_data),axis=0)
        kmeans = KMeans(n_clusters=1)  # 'k' 是你想要的聚类数量
        # 训练模型
        kmeans.fit(train_data)
        # 获取聚类中心
        center = kmeans.cluster_centers_[0]
        center = torch.Tensor(center)
        # calculate the farest distance
        radius = ((train_data - center)**2).sum(dim=1).max().sqrt()
        print("radius,radius",radius)

        min_radius_log = np.log10(1e-3)
        max_radius_log = np.log10(radius.item() * 1)
        # *****************************************************************************************
        # generate 100 points in log space 
        radii_log = np.linspace(max_radius_log, min_radius_log, self.interval)
        # convert back to linear space
        radii = 10 ** radii_log

    
        # calculate the number of samples for each radius
        num_samples_per_radius_l = []
        for r in radii:
            # calculate the log surface area for the current radius
            # convert it back to the original scale
            # calculate the number of samples
            num_samples = int(self.base_num_samples * r // 2)
            num_samples_per_radius_l.append(num_samples)
        

        # *****************************************************************************************
        radius = radius.item()
        # radii = [radius*1.1, radius, radius / 2, radius / 4, radius / 10, 1e-3]  # radii at which to sample points
        radii = [ radius / 4, radius / 10, 1e-3]  # radii at which to sample points
        # num_samples_per_radius_l = [500, 500, 500, 500, 500, 500]  # number of samples per radius
        aaa = 200
        num_samples_per_radius_l = [aaa, aaa, aaa, aaa, aaa, aaa]  # number of samples per radius
        print("num_samples_per_radius_l",radii)
        print("num_samples_per_radius_l",num_samples_per_radius_l)
        # list to store samples at all radii
        high_bom_samples = []

        for i in range(len(radii)):
            r = radii[i]

            num_samples_per_radius = num_samples_per_radius_l[i]
            # sample points on the sphere with radius r
            samples = torch.randn(num_samples_per_radius, 512)
            samples = samples / samples.norm(dim=1, keepdim=True) * r

            high_bom_samples.append(samples)

            # concatenate samples from all radii
            high_bom = torch.cat(high_bom_samples, dim=0)

            high_bom = high_bom.cpu().detach().numpy()

        print("shape", high_bom.shape)
        
        # calculate the distance of each training point to the center
        train_data_distances = ((train_data - center)**2).sum(dim=1).sqrt().cpu().detach().numpy()

        # for each radius, find the training data points close to it and add them to the high_bom
        epsilon = 1e-2  # the threshold for considering a point is close to the radius
        for r in radii:
            close_points_indices = np.where(np.abs(train_data_distances - r) < epsilon)[0]
            close_points = train_data[close_points_indices].cpu().detach().numpy()
            high_bom = np.concatenate((high_bom, close_points), axis=0)

      
        return high_bom
    
    def skeleton_gen_use_perturb(self, _epsilon=1e-2, _per=0.7):
        """
        find the nearest training data for each radius, 
        and then generate new proxes by this add perturbation on these nearest training data
        """
        torch.manual_seed(0)  # freeze the random seed
        torch.cuda.manual_seed_all(0)
        np.random.seed(0)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        epsilon = _epsilon

        train_data = self.data_provider.train_representation(epoch=self.epoch)
        # border_data = self.data_provider.border_representation(epoch=self.epoch)
        # train_data = np.concatenate((train_data,border_data),axis=0)
        train_data = torch.Tensor(train_data)
        center = train_data.mean(dim=0)

        # calculate the furthest distance
        max_radius = ((train_data - center)**2).sum(dim=1).max().sqrt().item()
        min_radius = max_radius  * _per  # this is your minimum radius
     
        
        # interval = int(max_radius * 12.8) #MINNIST and CIFAR 10 
        interval = int(max_radius * 12.8)
        print("max_radius", max_radius,"interval",interval)

        # split the interval between max_radius and min_radius into 100 parts
        radii = np.linspace(max_radius, min_radius, interval)


        high_bom_samples = []
        train_data_distances = ((train_data - center)**2).sum(dim=1).sqrt().cpu().detach().numpy()
        print(train_data_distances)
  
        for r in radii:
        
            # find the training data that is close to the current radius
            close_points_indices = np.where(np.abs(train_data_distances - r) < epsilon)[0]
            close_points = train_data[close_points_indices]
    
            # calculate the unit vector from center to the points
            direction_to_center = (close_points - center) / torch.norm(close_points - center, dim=1, keepdim=True)
    
            # add a small perturbation along the direction to the center to get the proxies on the sphere with radius r
            noise = direction_to_center * (epsilon)
            # noise = direction_to_center * torch.randn_like(close_points) * epsilon
            proxies = (close_points + noise).cpu().detach().numpy()
    
            # add the proxies to the skeleton
            high_bom_samples.append(proxies)
        
        high_bom = np.concatenate(high_bom_samples, axis=0)

        return high_bom
    
    def gen_skeleton_by_center(self,):
        train_data = self.data_provider.train_representation(self.epoch)
        kmeans = KMeans(n_clusters=1)  # 'k' 是你想要的聚类数量
        # 训练模型
        kmeans.fit(train_data)
        # 获取聚类中心
        centers = kmeans.cluster_centers_
        return 

    
    def create_decreasing_array(self,min_val, max_val, levels, factor=0.8):
        # Calculate the total range
        range_val = max_val - min_val

        # Create an array with the specified number of levels
        level_indices = np.arange(levels)

        # Apply the factor to the levels
        scaled_levels = factor ** level_indices

        # Scale the values to fit within the range
        scaled_values = scaled_levels * range_val / np.max(scaled_levels)

        # Shift the values to start at the min_val
        final_values = max_val - scaled_values

        return final_values


class CosineKMeans:
    def __init__(self, n_clusters=2, max_iter=100, tol=1e-4):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.tol = tol
        self.centers = None

    def fit(self, data, pred_data):
        # 确保 pred_data 和 data 有相同数量的样本
        assert len(pred_data) == len(data), "pred_data 和 data 必须有相同数量的样本"

        # 随机初始化聚类中心
        indices = np.random.choice(len(pred_data), self.n_clusters, replace=False)
        self.pred_centers = pred_data[indices]
        self.centers = data[indices]

        for _ in range(self.max_iter):
            # 计算余弦相似度
            similarities = cosine_similarity(pred_data, self.pred_centers)
            
            # 分配样本到最近的聚类中心
            labels = np.argmax(similarities, axis=1)

            # 更新聚类中心
            new_centers = np.array([data[labels == i].mean(axis=0) for i in range(self.n_clusters) if len(data[labels == i]) > 0])
            
            # 检查收敛
            if len(new_centers) == len(self.centers) and np.all(np.linalg.norm(new_centers - self.centers, axis=1) < self.tol):
                break

            self.centers = new_centers

        return labels, self.centers

class CenterSkeletonGenerator:
    """SkeletonGenerator except allows for generate skeleton"""
    def __init__(self, data_provider, epoch,threshold=0.5,min_cluster=500):
        """

        """
        self.data_provider = data_provider
        self.epoch = epoch
        self.threshold = threshold
        self.min_cluster = min_cluster
    
    def gen_center(self,data,pred_data,k=2):
        """
        """
        kmeans = KMeans(n_clusters=k)  
        kmeans.fit(data)
        centers = kmeans.cluster_centers_
        labels = kmeans.labels_
        radii = []
        # kmeans = CosineKMeans(n_clusters=3)
        # labels, centers = kmeans.fit(data,pred_data)
        for i in range(k):
            cluster_data = data[labels == i]
            if len(cluster_data) > 0:
                # calculate each sample distance to center
                distances = np.sqrt(((cluster_data - centers[i]) ** 2).sum(axis=1))
                radii.append(np.max(distances))
            else:
                radii.append(0)
        
        return centers,labels,radii
    
    def if_need_split(self, data):
        if len(data) < self.min_cluster:
            return False

        kmeans = KMeans(n_clusters=2) 
        kmeans.fit(data)
        labels = kmeans.labels_

        dunn_index = self.dunns_index(data, labels)
        print(dunn_index)
        return dunn_index < self.threshold
    
    def dunns_index(self, X, labels):
        distance_matrix = euclidean_distances(X)

        inter_cluster_distances = []
        intra_cluster_distances = []

        unique_labels = np.unique(labels)

        # Check if we have at least two clusters
        if len(unique_labels) < 2:
            return float('inf')  # Ineligible for splitting

        # Compute maximal intra-cluster distance
        for label in unique_labels:
            members = np.where(labels == label)[0]
            if len(members) <= 1:  # Skip clusters with only one member
                continue
            pairwise_distances = distance_matrix[np.ix_(members, members)]
            intra_cluster_distances.append(np.max(pairwise_distances))

        if not intra_cluster_distances:  # No eligible clusters found
            return float('inf')

        max_intra_cluster_distance = max(intra_cluster_distances)

        # Compute minimal inter-cluster distance
        for i in range(len(unique_labels)):
            for j in range(i+1, len(unique_labels)):
                members_i = np.where(labels == unique_labels[i])[0]
                members_j = np.where(labels == unique_labels[j])[0]
                pairwise_distances = distance_matrix[np.ix_(members_i, members_j)]
                inter_cluster_distances.append(np.min(pairwise_distances))

        if not inter_cluster_distances:  # No eligible clusters found
            return float('inf')

        return min(inter_cluster_distances) / max_intra_cluster_distance
    
    def recursive_clustering(self, data, pred_data,k=2, current_index=0, cluster_indices=None, original_indices=None):
        if cluster_indices is None:
            cluster_indices = np.zeros(data.shape[0], dtype=int)
            original_indices = np.arange(data.shape[0])

        centers, labels, radii = self.gen_center(data, pred_data, k=k)
        all_centers = list(centers)
        all_radii = list(radii)
        # next_index = current_index + 1
        next_index = current_index  # 从当前索引开始

        for label in set(labels):
            cluster_mask = (labels == label)
            cluster_data = data[cluster_mask]
            cluster__pred_data = pred_data[cluster_mask]
            cluster_original_indices = original_indices[cluster_mask]
            if len(cluster_data):
                if self.if_need_split(cluster_data):
                    sub_centers, sub_radii, used_index = self.recursive_clustering(
                        cluster_data,cluster__pred_data, k=2, current_index=next_index,
                        cluster_indices=cluster_indices, 
                        original_indices=cluster_original_indices
                    )
                    all_centers.extend(sub_centers)
                    all_radii.extend(sub_radii)
                    next_index = used_index 
                else:
                    cluster_indices[cluster_original_indices] = next_index
                    next_index += 1
                    # print("current_index",next_index)

        return all_centers, all_radii, next_index

    def gen_first_level_centers(self, data, pred_data, k=10):
        pred = pred_data.argmax(axis=1)
        centers = []

        radii = []

        for i in range(k):
            # Find the indices of data points in the current cluster
            indices = np.where(pred == i)[0]

            # Calculate the mean (center) of these points
            cluster_center = data[indices].mean(axis=0)
            centers.append(cluster_center)

            # Calculate the radius of the cluster
            if len(indices) > 0:
                distances = np.linalg.norm(data[indices] - cluster_center, axis=1)
                cluster_radius = distances.max()
            else:
                cluster_radius = 0

            radii.append(cluster_radius)
        return centers, pred, radii
        

    
    def center_skeleton_genertaion(self):
        # Initial centers
        data = self.data_provider.train_representation(self.epoch)
        data = data.reshape(data.shape[0], data.shape[1])
        pred_data = self.data_provider.get_pred(self.epoch, data)

        next_index = 0  # 从当前索引开始

        # pca = PCA(n_components=2)
        # data = pca.fit_transform(data)

        # pca = PCA(n_components=2)
        # data = pca.fit_transform(data)
        centers_c, _, radii_c = self.gen_center(data,pred_data,k=1)
        centers_n, labels,radii_n = self.gen_first_level_centers(data,pred_data,k=10)
        print("finished init, start generate proxy")

        # Recursive clustering
        # Recursive clustering with initial split into 10 clusters
        all_centers = []
        all_radii = []  # 存储所有簇的最大半径
        cluster_indices = np.zeros(data.shape[0], dtype=int)
        for label in range(len(labels)):
            # print("llllabel")
            cluster_mask = (labels == label)
            cluster_data = data[cluster_mask]
            cluster_pred_data = pred_data[cluster_mask]
            cluster_original_indices = np.where(cluster_mask)[0]
            if len(cluster_data):
                sub_centers, sub_radii, used_index = self.recursive_clustering(
                    cluster_data, cluster_pred_data,k=2, current_index=next_index,
                    cluster_indices=cluster_indices, 
                    original_indices=cluster_original_indices
                  
                )
                
                all_centers.extend(sub_centers)
                all_radii.extend(sub_radii)
                next_index = used_index
            
        all_centers = np.array(all_centers)
        all_radii = np.array(all_radii)

        centers = np.concatenate((centers_c,centers_n,all_centers))
        # centers = pca.inverse_transform(centers)

                                          
        return centers,np.concatenate((radii_c, radii_n, all_radii), axis=0), cluster_indices
    



class SpectralClustringProxyGenerator:
    """Use Spectral clustering generate proxies"""
    def __init__(self, data_provider, epoch,threshold=0.5,min_cluster=500):
        """

        """
        self.data_provider = data_provider
        self.epoch = epoch
        self.threshold = threshold
        self.min_cluster = min_cluster
        self.data = data_provider.train_representation(epoch = self.epoch)

        def pca_(self):
            pca = PCA(n_components=50)
            reduced_data = pca.fit_transform(self.data)


class HierarchicalClusteringProxyGenerator:

    """ Use Hierachical Clusering generate proxies"""
    def __init__(self, data_provider, epoch, threshold=0.5):
        self.data_provider = data_provider
        self.threshold = threshold
        self.epoch = epoch

    def hierarchical_clustering_analysis(self, data):
        # 执行层次聚类
        clustering = AgglomerativeClustering(n_clusters=10)
        clustering.fit(data)

        # 初始化每个数据点所属的簇
        n_samples = len(data)
        cluster_membership = {i: [i] for i in range(n_samples)}
        merges = []
        centers = []
        radiuss = []
        for merge_step, (cluster_1_idx, cluster_2_idx) in enumerate(clustering.children_):
            # 这些索引代表合并的簇
            new_cluster = cluster_membership[cluster_1_idx] + cluster_membership[cluster_2_idx]

            # 更新簇的成员
            cluster_membership[n_samples + merge_step] = new_cluster

            # 计算新簇的中心和半径
            new_cluster_data = data[new_cluster]
            center = np.mean(new_cluster_data, axis=0)
            radius = np.max(np.linalg.norm(new_cluster_data - center, axis=1))
            centers.append(center)
            radiuss.append(radius)

            # 记录合并的信息
            merges.append({
                'merge_step': merge_step,
                'merged_clusters': (cluster_1_idx, cluster_2_idx),
                'new_cluster_center': center,
                'new_cluster_radius': radius,
                'new_cluster_members': new_cluster
            })

        return centers,radiuss,merges
    
    def proxy_generation(self):

        data = self.data_provider.train_representation(self.epoch)
        data = data.reshape(data.shape[0], data.shape[1])
        sampled_data = data[np.random.choice(data.shape[0], size=1000, replace=False)]
        centers,radius,merges = self.hierarchical_clustering_analysis(sampled_data)
        return np.array(centers),radius,merges


        
        

   
    


